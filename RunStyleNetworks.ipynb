{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91a2463a-9083-4217-b9a2-bf6a433e90a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import vgg19\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from PIL import Image\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "matplotlib.rcParams[\"axes.grid\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05da4b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf的版本\n",
    "tf.__version__ == '2.12.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d5cad4-58b8-4a1d-a527-669655381927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, dim=None, resize=False):\n",
    "    img = Image.open(image_path)\n",
    "    if dim:\n",
    "        if resize:\n",
    "            img = img.resize(dim)\n",
    "        else:\n",
    "            img.thumbnail(dim)\n",
    "    img = img.convert(\"RGB\")\n",
    "    return np.array(img)\n",
    "\n",
    "# 使用该函数从url导入图片\n",
    "def load_url_image(url, dim=None, resize=False):\n",
    "    img_request = requests.get(url)\n",
    "    img = Image.open(BytesIO(img_request.content))\n",
    "    if dim:\n",
    "        if resize:\n",
    "            img = img.resize(dim)\n",
    "        else:\n",
    "            img.thumbnail(dim)\n",
    "    img = img.convert(\"RGB\")\n",
    "    return np.array(img)\n",
    "\n",
    "def array_to_img(array):\n",
    "    array = np.array(array, dtype=np.uint8)\n",
    "    if np.ndim(array) > 3:\n",
    "        assert array.shape[0] == 1\n",
    "        array = array[0]\n",
    "    return Image.fromarray(array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b3f9f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d822d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = [\"block4_conv2\"]\n",
    "style_layers = [\n",
    "    \"block1_conv1\",\n",
    "    \"block2_conv1\",\n",
    "    \"block3_conv1\",\n",
    "    \"block4_conv1\",\n",
    "    \"block5_conv1\",\n",
    "]\n",
    "content_layers_weights = [1]\n",
    "style_layers_weights = [1] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4deabf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算感知损失函数\n",
    "class LossModel:\n",
    "    def __init__(self, pretrained_model, content_layers, style_layers):\n",
    "        self.model = pretrained_model\n",
    "        self.content_layers = content_layers\n",
    "        self.style_layers = style_layers\n",
    "        self.loss_model = self.get_model()\n",
    "\n",
    "    def get_model(self):\n",
    "        self.model.trainable = False\n",
    "        layer_names = self.style_layers + self.content_layers\n",
    "        outputs = [self.model.get_layer(name).output for name in layer_names]\n",
    "        new_model = Model(inputs=self.model.input, outputs=outputs)\n",
    "        return new_model\n",
    "\n",
    "    def get_activations(self, inputs):\n",
    "        inputs = inputs * 255.0\n",
    "        style_length = len(self.style_layers)\n",
    "        outputs = self.loss_model(vgg19.preprocess_input(inputs))\n",
    "        style_output, content_output = outputs[:style_length], outputs[style_length:]\n",
    "        content_dict = {\n",
    "            name: value for name, value in zip(self.content_layers, content_output)\n",
    "        }\n",
    "        style_dict = {\n",
    "            name: value for name, value in zip(self.style_layers, style_output)\n",
    "        }\n",
    "        return {\"content\": content_dict, \"style\": style_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16712926",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_model = LossModel(vgg, content_layers, style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9c0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内容损失函数\n",
    "def content_loss(placeholder, content, weight):\n",
    "    assert placeholder.shape == content.shape\n",
    "    return weight * tf.reduce_mean(tf.square(placeholder - content))\n",
    "\n",
    "# 格拉姆矩阵 --> 用于风格损失函数\n",
    "def gram_matrix(x):\n",
    "    gram = tf.linalg.einsum(\"bijc,bijd->bcd\", x, x)\n",
    "    return gram / tf.cast(x.shape[1] * x.shape[2] * x.shape[3], tf.float32)\n",
    "\n",
    "# 风格损失函数\n",
    "def style_loss(placeholder, style, weight):\n",
    "    assert placeholder.shape == style.shape\n",
    "    s = gram_matrix(style)\n",
    "    p = gram_matrix(placeholder)\n",
    "    return weight * tf.reduce_mean(tf.square(s - p))\n",
    "\n",
    "# 感知损失函数\n",
    "def preceptual_loss(\n",
    "    predicted_activations,\n",
    "    content_activations,\n",
    "    style_activations,\n",
    "    content_weight,\n",
    "    style_weight,\n",
    "    content_layers_weights,\n",
    "    style_layer_weights,\n",
    "):\n",
    "    pred_content = predicted_activations[\"content\"]\n",
    "    pred_style = predicted_activations[\"style\"]\n",
    "    c_loss = tf.add_n(\n",
    "        [\n",
    "            content_loss(\n",
    "                pred_content[name], content_activations[name], content_layers_weights[i]\n",
    "            )\n",
    "            for i, name in enumerate(pred_content.keys())\n",
    "        ]\n",
    "    )\n",
    "    c_loss = c_loss * content_weight\n",
    "    s_loss = tf.add_n(\n",
    "        [\n",
    "            style_loss(\n",
    "                pred_style[name], style_activations[name], style_layer_weights[i]\n",
    "            )\n",
    "            for i, name in enumerate(pred_style.keys())\n",
    "        ]\n",
    "    )\n",
    "    s_loss = s_loss * style_weight\n",
    "    return c_loss + s_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a328e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速风格网络结构\n",
    "class ReflectionPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "        self.padding = tuple(padding)\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        padding_width, padding_height = self.padding\n",
    "        return tf.pad(\n",
    "            input_tensor,\n",
    "            [\n",
    "                [0, 0],\n",
    "                [padding_height, padding_height],\n",
    "                [padding_width, padding_width],\n",
    "                [0, 0],\n",
    "            ],\n",
    "            \"REFLECT\",\n",
    "        )\n",
    "\n",
    "    \n",
    "class InstanceNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(InstanceNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch, rows, cols, channels = [i for i in inputs.get_shape()]\n",
    "        mu, var = tf.nn.moments(inputs, [1, 2], keepdims=True)\n",
    "        shift = tf.Variable(tf.zeros([channels]))\n",
    "        scale = tf.Variable(tf.ones([channels]))\n",
    "        epsilon = 1e-3\n",
    "        normalized = (inputs - mu) / tf.sqrt(var + epsilon)\n",
    "        return scale * normalized + shift\n",
    "\n",
    "    \n",
    "class ConvLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=1, **kwargs):\n",
    "        super(ConvLayer, self).__init__(**kwargs)\n",
    "        self.padding = ReflectionPadding2D([k // 2 for k in kernel_size])\n",
    "        self.conv2d = tf.keras.layers.Conv2D(filters, kernel_size, strides)\n",
    "        self.bn = InstanceNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.padding(inputs)\n",
    "        x = self.conv2d(x)\n",
    "        x = self.bn(x)\n",
    "        return x    \n",
    "    \n",
    "    \n",
    "class ResidualLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, **kwargs):\n",
    "        super(ResidualLayer, self).__init__(**kwargs)\n",
    "        self.conv2d_1 = ConvLayer(filters, kernel_size)\n",
    "        self.conv2d_2 = ConvLayer(filters, kernel_size)\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        residual = inputs\n",
    "        x = self.conv2d_1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.add([x, residual])\n",
    "        return x\n",
    "    \n",
    "class UpsampleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=1, upsample=2, **kwargs):\n",
    "        super(UpsampleLayer, self).__init__(**kwargs)\n",
    "        self.upsample = tf.keras.layers.UpSampling2D(size=upsample)\n",
    "        self.padding = ReflectionPadding2D([k // 2 for k in kernel_size])\n",
    "        self.conv2d = tf.keras.layers.Conv2D(filters, kernel_size, strides)\n",
    "        self.bn = InstanceNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.upsample(inputs)\n",
    "        x = self.padding(x)\n",
    "        x = self.conv2d(x)\n",
    "        return self.bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8305d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(StyleTransferModel, self).__init__(name=\"StyleTransferModel\", **kwargs)\n",
    "        self.conv2d_1 = ConvLayer(\n",
    "            filters=32, kernel_size=(9, 9), strides=1, name=\"conv2d_1_32\"\n",
    "        )\n",
    "        self.conv2d_2 = ConvLayer(\n",
    "            filters=64, kernel_size=(3, 3), strides=2, name=\"conv2d_2_64\"\n",
    "        )\n",
    "        self.conv2d_3 = ConvLayer(\n",
    "            filters=128, kernel_size=(3, 3), strides=2, name=\"conv2d_3_128\"\n",
    "        )\n",
    "        self.res_1 = ResidualLayer(filters=128, kernel_size=(3, 3), name=\"res_1_128\")\n",
    "        self.res_2 = ResidualLayer(filters=128, kernel_size=(3, 3), name=\"res_2_128\")\n",
    "        self.res_3 = ResidualLayer(filters=128, kernel_size=(3, 3), name=\"res_3_128\")\n",
    "        self.res_4 = ResidualLayer(filters=128, kernel_size=(3, 3), name=\"res_4_128\")\n",
    "        self.res_5 = ResidualLayer(filters=128, kernel_size=(3, 3), name=\"res_5_128\")\n",
    "        self.deconv2d_1 = UpsampleLayer(\n",
    "            filters=64, kernel_size=(3, 3), name=\"deconv2d_1_64\"\n",
    "        )\n",
    "        self.deconv2d_2 = UpsampleLayer(\n",
    "            filters=32, kernel_size=(3, 3), name=\"deconv2d_2_32\"\n",
    "        )\n",
    "        self.deconv2d_3 = ConvLayer(\n",
    "            filters=3, kernel_size=(9, 9), strides=1, name=\"deconv2d_3_3\"\n",
    "        )\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv2d_1(inputs)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.res_1(x)\n",
    "        x = self.res_2(x)\n",
    "        x = self.res_3(x)\n",
    "        x = self.res_4(x)\n",
    "        x = self.res_5(x)\n",
    "        x = self.deconv2d_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.deconv2d_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.deconv2d_3(x)\n",
    "        x = (tf.nn.tanh(x) + 1) * (255.0 / 2)\n",
    "        return x\n",
    "\n",
    "    def print_shape(self, inputs):\n",
    "        print(inputs.shape)\n",
    "        x = self.conv2d_1(inputs)\n",
    "        print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d_2(x)\n",
    "        print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d_3(x)\n",
    "        print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.res_1(x)\n",
    "        print(x.shape)\n",
    "        x = self.res_2(x)\n",
    "        print(x.shape)\n",
    "        x = self.res_3(x)\n",
    "        print(x.shape)\n",
    "        x = self.res_4(x)\n",
    "        print(x.shape)\n",
    "        x = self.res_5(x)\n",
    "        print(x.shape)\n",
    "        x = self.deconv2d_1(x)\n",
    "        print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.deconv2d_2(x)\n",
    "        print(x.shape)\n",
    "        x = self.relu(x)\n",
    "        x = self.deconv2d_3(x)\n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ff8bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (256, 256, 3)\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3165e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_model = StyleTransferModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "857dfbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256, 256, 3)\n",
      "(1, 256, 256, 32)\n",
      "(1, 128, 128, 64)\n",
      "(1, 64, 64, 128)\n",
      "(1, 64, 64, 128)\n",
      "(1, 64, 64, 128)\n",
      "(1, 64, 64, 128)\n",
      "(1, 64, 64, 128)\n",
      "(1, 64, 64, 128)\n",
      "(1, 128, 128, 64)\n",
      "(1, 256, 256, 32)\n",
      "(1, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "style_model.print_shape(tf.zeros(shape=(1, *input_shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec2931",
   "metadata": {},
   "source": [
    "## 这里选择风格网络与内容图\n",
    "\n",
    "- 以上代码无需重复运行，若要运行多个结果，从以下代码开始重复即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2e77fd6-7456-4231-95d5-b482a9ef32e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 你要导入的模型文件路径\n",
    "save_path = \"./scream/rainPrincess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20ee9ecb-1ee2-4e7e-a7b6-b9ea2374b4f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights ...\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(os.path.join(save_path, \"model_checkpoint.ckpt.index\")):\n",
    "    style_model.load_weights(os.path.join(save_path, \"model_checkpoint.ckpt\"))\n",
    "    print(\"loading weights ...\")\n",
    "else:\n",
    "    print(\"no weights found ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56970352-049c-4f45-a60b-4359fd0d8a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 要生成的内容图的图片路径\n",
    "test_image = load_image(\"images/content/wit1.jpg\")\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "test_image = test_image.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d84d66ff-b25f-43dd-b81f-d5ed2b06983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_image = style_model(test_image)\n",
    "predicted_image = np.clip(predicted_image, 0, 255)\n",
    "predicted_image = predicted_image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b3c744d-2bb7-43ab-b574-c3e3dd07ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_image.astype(np.uint8)\n",
    "test_output = tf.squeeze(test_output).numpy()\n",
    "predicted_output = tf.squeeze(predicted_image).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c41486e-fe00-4678-b5d2-a1ceb579a54f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 显示图片\n",
    "plt.imshow(predicted_output)\n",
    "\n",
    "# 关闭坐标轴\n",
    "plt.axis('off')\n",
    "\n",
    "# 保存图片\n",
    "plt.savefig('./images/output/result.png')\n",
    "\n",
    "# 展示图片 (jupyter 可能无法正常显示，建议保存图片查看)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
